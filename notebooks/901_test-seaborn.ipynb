{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from strictyaml import YAMLValidationError\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import sklearn\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_context(\"poster\")\n",
    "sns.set(rc={'figure.figsize': (16, 9.)})\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_rows\", 120)\n",
    "pd.set_option(\"display.max_columns\", 120)\n",
    "\n",
    "import textacy\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ana_consult import _, __version__\n",
    "from ana_consult.ac_conf import AnaConsultConf\n",
    "\n",
    "APP_NAME = \"mtes_analyze\"\n",
    "logger = logging.getLogger(APP_NAME + \".cluster\")\n",
    "cfg = \".mtes.yaml\"\n",
    "logger.info(_(\"Getting configuration data from %s\"), cfg)\n",
    "try:\n",
    "    config = AnaConsultConf(cfg)\n",
    "except YAMLValidationError:\n",
    "    logger.critical(_(\"Incorrect content in YAML configuration %s\"), cfg)\n",
    "    sys.exit(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare NLP processing\n",
    "logger.info(_(\"Preparing NLP text processing\"))\n",
    "fr_nlp = textacy.load_spacy_lang(\n",
    "    \"fr_core_news_sm\", disable=(\"tagger\", \"parser\", \"ner\")\n",
    ")\n",
    "logger.info(_(\"NLP pipeline: %s\"), fr_nlp.pipe_names)\n",
    "# Adjust stopwords for this specific topic\n",
    "fr_nlp.Defaults.stop_words |= {\"y\", \"france\", \"italie\"}\n",
    "fr_nlp.Defaults.stop_words -= {\"contre\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "corpus_file = Path.home() / (\n",
    "    \"ana_consult/data/interim/\" + config.consultation_name + \"_doc.pkl\"\n",
    ")\n",
    "logger.info(_(\"Loading corpus from %s\"), corpus_file)\n",
    "corpus = textacy.Corpus.load(fr_nlp, corpus_file)\n",
    "logger.info(_(\"Document size: %s\"), corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define vectorizer parameters\n",
    "logger.info(_(\"Simplifying corpus\"))\n",
    "doc_lemma = pd.DataFrame(\n",
    "    [\n",
    "        [\n",
    "            \" \".join(\n",
    "                list(\n",
    "                    doc._.to_terms_list(\n",
    "                        ngrams=1,\n",
    "                        entities=False,\n",
    "                        normalize=\"lemma\",\n",
    "                        as_strings=True,\n",
    "                        filter_stops=True,\n",
    "                        filter_punct=True,\n",
    "                        filter_nums=True,\n",
    "                    )\n",
    "                )\n",
    "            ),\n",
    "            doc._.meta[\"opinion\"],\n",
    "        ]\n",
    "        for doc in corpus[:1000000]\n",
    "    ],\n",
    "    columns=[\"text\", \"opinion\"],\n",
    ")\n",
    "print(doc_lemma.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_lemma_cls = doc_lemma.dropna()\n",
    "print(doc_lemma_cls.opinion.describe())\n",
    "true_labels = [0 if d == \"Favorable\" else 1 for d in doc_lemma_cls.opinion]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_df=0.9, min_df=0.05, stop_words=None, use_idf=True, ngram_range=(1, 3)\n",
    ")\n",
    "# Fit vectoriser to NLP processed column\n",
    "logger.info(_(\"Fitting TF-IDF vectorizer to NLP data\"))\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(doc_lemma_cls.text)\n",
    "terms = np.array(tfidf_vectorizer.get_feature_names())\n",
    "logger.info(_(\"TF-IDF (n_samples, n_features): %s\"), tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_tfidf_feats(row, features, top_n=25):\n",
    "    ''' Get top n tfidf values in row and return them with their corresponding feature names.'''\n",
    "    topn_ids = np.argsort(row)[::-1][:top_n]\n",
    "    top_feats = [(features[i], row[i]) for i in topn_ids]\n",
    "    df = pd.DataFrame(top_feats)\n",
    "    df.columns = ['feature', 'tfidf']\n",
    "    return df\n",
    "\n",
    "def top_feats_in_doc(Xtr, features, row_id, top_n=25):\n",
    "    ''' Top tfidf features in specific document (matrix row) '''\n",
    "    row = np.squeeze(Xtr[row_id].toarray())\n",
    "    return top_tfidf_feats(row, features, top_n)\n",
    "\n",
    "def top_mean_feats(Xtr, features, grp_ids=None, min_tfidf=0.1, top_n=25):\n",
    "    ''' Return the top n features that on average are most important amongst documents in rows\n",
    "        indentified by indices in grp_ids. '''\n",
    "    if grp_ids:\n",
    "        D = Xtr[grp_ids].toarray()\n",
    "    else:\n",
    "        D = Xtr.toarray()\n",
    "\n",
    "    D[D < min_tfidf] = 0\n",
    "    tfidf_means = np.mean(D, axis=0)\n",
    "    return top_tfidf_feats(tfidf_means, features, top_n)\n",
    "\n",
    "def top_feats_by_class(Xtr, y, features, min_tfidf=0.1, top_n=25):\n",
    "    ''' Return a list of dfs, where each df holds top_n features and their mean tfidf value\n",
    "        calculated across documents with the same class label. '''\n",
    "    dfs = []\n",
    "    labels = np.unique(y)\n",
    "    for label in labels:\n",
    "        ids = np.where(y==label)\n",
    "        feats_df = top_mean_feats(Xtr, features, ids, min_tfidf=min_tfidf, top_n=top_n)\n",
    "        feats_df.label = label\n",
    "        dfs.append(feats_df)\n",
    "    return dfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_feats_by_class(tfidf_matrix, doc_lemma_cls.opinion, terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tfidf_classfeats_h(dfs):\n",
    "    ''' Plot the data frames returned by the function plot_tfidf_classfeats(). '''\n",
    "    fig = plt.figure(figsize=(12, 9), facecolor=\"w\")\n",
    "    x = np.arange(len(dfs[0]))\n",
    "    for i, df in enumerate(dfs):\n",
    "        ax = fig.add_subplot(1, len(dfs), i+1)\n",
    "        ax.spines[\"top\"].set_visible(False)\n",
    "        ax.spines[\"right\"].set_visible(False)\n",
    "        ax.set_frame_on(False)\n",
    "        ax.get_xaxis().tick_bottom()\n",
    "        ax.get_yaxis().tick_left()\n",
    "        ax.set_xlabel(\"Mean Tf-Idf Score\", labelpad=16, fontsize=14)\n",
    "        ax.set_title(\"label = \" + str(df.label), fontsize=16)\n",
    "        ax.ticklabel_format(axis='x', style='sci', scilimits=(-2,2))\n",
    "        ax.barh(x, df.tfidf, align='center', color='#3F5D7D')\n",
    "        ax.set_yticks(x)\n",
    "        ax.set_ylim([-1, x[-1]+1])\n",
    "        yticks = ax.set_yticklabels(df.feature)\n",
    "        plt.subplots_adjust(bottom=0.09, right=0.97, left=0.15, top=0.95, wspace=0.52)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tfidf_classfeats_h(top_feats_by_class(tfidf_matrix, doc_lemma_cls.opinion, terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
